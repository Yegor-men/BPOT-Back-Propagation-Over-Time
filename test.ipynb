{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28dd61cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f100e5208f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6209c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming Spikes: tensor([1., 1., 0., 1., 0.])\n",
      "Out Spikes: tensor([0., 1.], device='cuda:0')\n",
      "Learning Signal: tensor([0., 0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_in: int,\n",
    "        num_out: int,\n",
    "        threshold: float = 1,\n",
    "        mem_decay: float = 0.99,\n",
    "        ls_decay:float = 0,\n",
    "        lr: float = 1e-5,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> None:\n",
    "        self.num_in = num_in\n",
    "        self.num_out = num_out\n",
    "        \n",
    "        self.mem_decay = mem_decay\n",
    "        self.ls_decay = ls_decay\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.w = torch.randn(num_in, num_out).to(device)\n",
    "        self.mem = torch.zeros(num_out).to(self.w)\n",
    "        self.threshold = torch.ones(num_out).to(self.w) * threshold\n",
    "        \n",
    "        self.ls = torch.zeros_like(self.mem)\n",
    "        \n",
    "        self.in_trace = torch.zeros(num_in).to(self.w)\n",
    "    \n",
    "    def update_ls(\n",
    "        self,\n",
    "        learning_signal: torch.Tensor,\n",
    "    ) -> None:\n",
    "        assert learning_signal.size() == (self.num_out,), f\"Learning Signal update: expected size ({self.num_out},), got {learning_signal.size()} instead\"\n",
    "        learning_signal = learning_signal.to(self.ls)\n",
    "        \n",
    "        self.ls *= self.ls_decay\n",
    "        self.ls += learning_signal\n",
    "    \n",
    "    def backward(\n",
    "        self,\n",
    "    ) -> torch.Tensor:\n",
    "        ls = self.ls.unsqueeze(0)  # [1, out]\n",
    "        ls = torch.broadcast_to(ls, (self.num_in, self.num_out))  # [in, out]\n",
    "        \n",
    "        input_tensor = self.in_trace.unsqueeze(-1)  # [in, 1]\n",
    "        input_tensor = torch.broadcast_to(input_tensor, (self.num_in, self.num_out))  # [in, out]\n",
    "        \n",
    "        delta_w = ls * input_tensor  # d_ls/d_w = input\n",
    "        delta_input = ls * self.w  # d_ls/d_in = weight\n",
    "        \n",
    "        self.w += delta_w * self.lr\n",
    "        \n",
    "        passed_ls = delta_input.sum(dim=-1)\n",
    "        \n",
    "        return passed_ls\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        in_spikes: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        assert in_spikes.size() == (self.num_in,), f\"Forward: expected size ({self.num_in},), got {in_spikes.size()} instead\"\n",
    "        in_spikes = in_spikes.to(self.w)\n",
    "        \n",
    "        back_pass_ls = self.backward()\n",
    "        \n",
    "        self.in_trace *= self.mem_decay\n",
    "        self.in_trace += in_spikes\n",
    "        \n",
    "        current = in_spikes @ self.w\n",
    "        self.mem *= self.mem_decay\n",
    "        self.mem += current\n",
    "        \n",
    "        out_spikes = (self.mem >= self.threshold).float()\n",
    "        \n",
    "        self.mem -= out_spikes * self.threshold\n",
    "        \n",
    "        return out_spikes, back_pass_ls\n",
    "\n",
    "n_in, n_out = 5, 2\n",
    "test_layer = Layer(n_in, n_out)\n",
    "in_spikes = torch.rand(n_in).round()\n",
    "print(f\"Incoming Spikes: {in_spikes}\")\n",
    "out_spikes, learning_signals = test_layer.forward(in_spikes)\n",
    "print(f\"Out Spikes: {out_spikes}\")\n",
    "print(f\"Learning Signal: {learning_signals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 0 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0., -1.], device='cuda:0'), loss=2.0\n",
      "Timestep 1 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 2 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 3 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0., -1.], device='cuda:0'), loss=2.0\n",
      "Timestep 4 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 5 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 6 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 7 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0., -1.], device='cuda:0'), loss=2.0\n",
      "Timestep 8 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Timestep 9 - LS for last layer: tensor([ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1., -1.], device='cuda:0'), loss=3.0\n",
      "Signal for layer 0: tensor([ 0.2857, -8.5617, 15.6056,  1.4001, 23.2930], device='cuda:0'), loss=18.23858642578125\n",
      "Signal for layer 1: tensor([ 1.9157,  0.3754, -1.1677,  2.4967,  1.0701], device='cuda:0'), loss=36.872962951660156\n",
      "Signal for layer 2: tensor([1., 0., 0., 1., 0.], device='cuda:0'), loss=3.0\n",
      "Difference for training: -0.19694304466247559\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        self.layers = [Layer(100,100) for _ in range(3)]\n",
    "        self.layers[-1] = Layer(100,10)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        spike_input: torch.Tensor,\n",
    "        expected_output: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        spike_input = spike_input.to(self.layers[0].w)\n",
    "        expected_output = expected_output.to(self.layers[0].w)\n",
    "        \n",
    "        o, ls = self.layers[0].forward(spike_input)\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            o, ls = self.layers[i].forward(o)\n",
    "            self.layers[i-1].update_ls(ls)\n",
    "        \n",
    "        ls = o - expected_output\n",
    "        self.layers[-1].update_ls(ls)\n",
    "        \n",
    "        return ls\n",
    "\n",
    "    def get_learning_signals(\n",
    "        self,\n",
    "    ) -> list[torch.Tensor]:\n",
    "        ls = []        \n",
    "        for layer in self.layers:\n",
    "            ls.append(layer.ls)\n",
    "        return ls\n",
    "\n",
    "bpot = Network()\n",
    "\n",
    "start_l1_weight = bpot.layers[0].w.detach().clone()\n",
    "\n",
    "input_spikes = torch.rand(100).round()\n",
    "expected_output = torch.rand(10).round()\n",
    "\n",
    "num_timesteps = 10\n",
    "for i in range(num_timesteps):\n",
    "    ls = bpot.forward(input_spikes, expected_output)\n",
    "    print(f\"Timestep {i} - LS for last layer: {ls}, loss={ls.sum()}\")\n",
    "\n",
    "\n",
    "signals = bpot.get_learning_signals()\n",
    "for index, signal in enumerate(signals):\n",
    "    print(f\"Signal for layer {index}: {signal[:5]}, loss={signal.sum()}\")\n",
    "\n",
    "end_l1_weight = bpot.layers[0].w.detach().clone()\n",
    "\n",
    "print(f\"Difference for training: {(start_l1_weight-end_l1_weight).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
